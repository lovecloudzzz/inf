import os
import uuid
import re
from time import sleep, time
import requests as req
import bs4 as bs
from src.maps.hash_map import HashMap


def timer_func(func):

    def wrap_func(*args, **kwargs):
        start = time()
        result = func(*args, **kwargs)
        end = time()
        print(f'Function {func.__name__!r} executed in {(end - start):.4f}s')
        return result

    return wrap_func


def convert_to_word(string):
    for i in ".:;,«»°()":
        string = string.replace(i, "")
    temp = []
    for word in string.split():
        if word[0] in "абвгдеёжзийклмнопрстуфхцчшщъыьэюя" \
                      "АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ":
            temp.append(word)
        else:
            temp.append(None)
    return temp


def add_base_path(base_path):
    if os.path.exists(base_path):
        print("Создание не удалось: папка base_path уже создана.\n", end="")
    else:
        os.mkdir(base_path)
        print("Папка base_path создана.\n", end="")


def from_list_to_file(lis, path):
    with open(path, "w", encoding="utf-8") as file:
        for i in lis:
            file.write(f"{i[0]}\t{i[1]}\n")


def wiki_parser(url: str, base_path):
    base_path += "\\base_path"
    add_base_path(base_path)

    flag = True
    dirlist = os.listdir(base_path)
    path = base_path
    for i in dirlist:
        with open(os.path.join(base_path, i, "url.txt"), "r", encoding="utf-8") as url_file:
            if url_file.read() == url:
                flag = False
                path = os.path.join(path, i)
                print("_____поиск url завершен, url уже был обработан_____\n", end="")
                break
    print("поиск url завершен, url еще не был обработан\n", end="")
    if flag:
        path = os.path.join(path, uuid.uuid4().hex)
        os.mkdir(path)
        with open(os.path.join(path, "url.txt"), "w", encoding="utf-8") as url_file:
            url_file.write(url)
        text = req.request("GET", url).content
        with open(os.path.join(path, "content.bin"), "wb") as content_file:
            content_file.write(text)

        print("_____url обработан_____\n", end="")
    if not os.path.exists(os.path.join(path, "content.bin")):
        while True:
            sleep(0.1)
            if os.path.exists(os.path.join(path, "content.bin")):
                break
    with open(os.path.join(path, "content.bin"), "rb") as content_file:
        soup = bs.BeautifulSoup(content_file, "lxml")
        hash_map = HashMap()
        for string in soup.stripped_strings:
            for word in convert_to_word(string):
                if word is None:
                    continue
                try:
                    hash_map[word] += 1
                except KeyError:
                    hash_map[word] = 1
        from_list_to_file(sorted(hash_map, key=lambda x: x[0]), os.path.join(path, "words.txt"))
        href_list = []
        for tag in soup.find_all(href=re.compile("^/wiki/")):
            href_list.append("https://ru.wikipedia.org" + tag["href"])
        print("__________выполнение закончено__________\n", end="")
        return href_list


if __name__ == "__main__":
    wiki_parser('https://ru.wikipedia.org/wiki/Нартуо',
          r'D:\inf\5\src')
